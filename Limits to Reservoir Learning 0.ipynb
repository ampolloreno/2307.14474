{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92fa61f4",
   "metadata": {},
   "source": [
    "## Welcome to my page!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d48834",
   "metadata": {},
   "source": [
    "I'm currently a post-doc postdoctoral associate at JILA, doing machine learning research. In this series of notebooks, I'm going to walk you through a small example of how we might think about understanding the performance of AI as it scales. It's based off of my paper: https://arxiv.org/abs/2307.14474"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bec520e",
   "metadata": {},
   "source": [
    "Why should you care? A few reasons. If you're getting involved in AI research, I hope this can inspire you to try out some of your own ideas - at least get a sense of some experiments you can run. Second, this work generalizes a results from the physics of computation - namely that there is an exponential reduction in the achievable state space when circuits are made to be physical. Shannon showed this for classical circuits, Poulin et al. showed this for quantum circuits, and now I have shown this in a learning setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dcb668",
   "metadata": {},
   "source": [
    "We consider a very specific setting, that of reservoir computing. This kind of model can be used for generative AI (think chatgpt) or just learning a function (regression). Here is the pitch: the reservoir gives you some number of signals. If we assume this data is scarce, we might like to process it, so that out of $n$ outputs, we product $2^n$ outputs with digital logic that we assume is noiseless, and fast. Now we have $2^n$ signals - are they useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa4c29e",
   "metadata": {},
   "source": [
    "Without noise, of course. With noise, now we have a probability distribution on the output - $2^n$ output signals (which we assume to be bits), and so we can equivalently consider the signals to be samples from the bitstring probabilities (of which there are also $2^n$). Now we're set up to ask our question - does this exponentially large state space help?\n",
    "\n",
    "(By considering bits, we have assumed that our reccurent computation is expressible as some sum of Bernoulli random variables. This is reasonable, since we generally can imagine using a sum of biased coin flips to produce a distribution of our choice. We will currently ignore the discussion of efficient computability in this sense.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195845f9",
   "metadata": {},
   "source": [
    "Generally we should be skeptical - probabilistic Turing machines are not more powerful, even though they have the same exponential state space, but as we saw in the noiseless case it _does_ help. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18e143f",
   "metadata": {},
   "source": [
    "What we find is that, for a single bit, if we are unable to produce exponentially large changes in the bit probabilities, we are unable to usefully avoid the noise - bitstrings are generally confused with each other, and this means their signals are relatively useless. A sufficient condition for being unable to produce exponentially large changes in the probabilities is that the model is represented by a physical circuit - just like the ones considered by Shannon and Poulin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1b3729",
   "metadata": {},
   "source": [
    "Interestingly, this is mostly a result about analog computing in the presence of noise. This is the kind of setting we have in the reinforcement learning setting where the values we receive are stochastic, continuous values, where our algorithms receives the correct value in expectation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28baf2dc",
   "metadata": {},
   "source": [
    "Let's check out an example."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
